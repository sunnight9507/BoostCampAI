{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reported-heath",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "> - $\\mathrm{softmax}$ 함수는 **모델의 출력을 확률로 해석**할 수 있게 변환해 주는 **연산**\n",
    "> - **분류 문제**를 풀 때 선형모델과 $\\mathrm{softmax}$ 함수를 결합하여 예측한다.\n",
    "> - 출력 벡터 $\\mathbf{o}$ 에 $\\mathrm{softmax}$ 함수를 합성하면 확률벡터가 되므로 **특정 클래스 $k$에 속할 확률**로 해석할 수 있다.\n",
    "> - $$ \\mathrm{softmax} ( \\mathbf{o} ) = (\\frac{\\exp(o_1)}{\\sum_{k=1}^p \\exp(o_k)},\\cdots ,\\frac{\\exp(o_p)}{\\sum_{k=1}^p \\exp(o_k)}) $$\n",
    "> - 추론을 할 때는 **원-핫(one-hot) 벡터**로 최대값을 가진 주소만 1로 출력하는 연산을 사용해 $\\mathrm{softmax}$를 사용하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "individual-formula",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09003057, 0.24472847, 0.66524096],\n",
       "       [0.70538451, 0.25949646, 0.03511903]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(vec):\n",
    "    denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True)) # overflow 방지\n",
    "    numerator = np.sum(denumerator, axis=-1, keepdims=True)\n",
    "    val = denumerator / numerator\n",
    "    return val\n",
    "\n",
    "softmax(np.array([[1,2,3],[4,3,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-progress",
   "metadata": {},
   "source": [
    "### 신경망을 수식으로 분해\n",
    "\n",
    "> - 신경망은 **선형모델과 활성함수(activation function)를 합성한 함수**\n",
    "> - 활성함수 $\\sigma$는 비선형함수로 잠재벡터 $\\mathbf{z} = (z_1, \\cdots, z_q)$의 각 노드에 개별적으로 적용하여 새로운 잠재벡터 $\\mathbf{H} = (\\sigma(\\mathbf{z_1}), \\cdots, \\sigma(\\mathbf{z_n}))$\n",
    "> - 잠재벡터 $\\mathbf{H}$에서 가중치 행렬 $\\mathbf{W}$와 $\\mathbf{b}$를 통해 다시 선형변환을  하게 되므로 2층(2-layer)신경망이다.\n",
    "> - 이를 반복적으로 수행할 경우 여러층이 합성된 함수인 **다층 퍼셉트론(MLP)**라고 한다.\n",
    "> - $ l = 1,\\cdots,L$까지 순차적인 신경망 계산을 순전파(forward propagation)라 부른다.\n",
    "\n",
    "### Activation function\n",
    "\n",
    "> - 활성함수는 $ \\mathbb{R} $ 위에 정의된 **비선형(nonlinear) 함수**로서 딥러닝에서 매우 중요한 개념\n",
    "> - **활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없다.**\n",
    "> - 시그모이드 함수나 tanh 함수는 전통적으로 많이 쓰이던 활성함수지만 **딥러닝에선 `ReLU` 함수를 많이 쓰고 있다.**\n",
    "\n",
    "### 층을 여러개 쌓는 이유\n",
    "\n",
    "> - 층이 깊을수록 **목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 훨씬 빨리 줄어들어 좀 더 효율적으로 학습이 가능**하다.\n",
    "< - 층이 얇으면 필요한 뉴런의 숫자가 기하급수적으로 늘어나서 넓은(wide) 신경망이 되어야 한다.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "> - 딥러닝은 **역전파 알고리즘**을 이용해 각 층에 사용된 파라미터를 학습한다.\n",
    "> - 각 층 파라미터의 그레디언트 벡터는 윗층부터 역순으로 계산하게 된다.\n",
    "> $ l = L,\\cdots,1$ 순서로 연쇄법칙을 통해 그레디언트 벡터를 전달한다.\n",
    "> - 역전파 알고리즘은 합성함수 미분법인 **연쇄법칙(chain-rule) 기반 자동미분(auto-differentiation)**을 사용한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
