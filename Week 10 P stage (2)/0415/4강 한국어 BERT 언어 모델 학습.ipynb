{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stainless-indication",
   "metadata": {},
   "source": [
    "### 1. BERT 학습하기\n",
    "\n",
    "#### 1.1 BERT 모델 학습 단계\n",
    "\n",
    "> - 1) Tokenizer 만들기\n",
    "> - 2) 데이터셋 확보\n",
    "> - 3) Next sentence prediction (NSP)\n",
    "> - 4) Masking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "primary-malaysia",
   "metadata": {},
   "source": [
    "### 새로 학습한 내용\n",
    "\n",
    "> - PreTrainedTokenizer VS. PreTrainedTokenizerFast\n",
    "> - 일괄된 토큰을 처리할 때 상당한 속도 차이가 있다.\n",
    "> - T5, ALBERT, CamemBERT, XLMRoBERTa XLNet은 SentencePiece-based tokenizers이므로 Fast는 이용할 수 없다.\n",
    "> - [참고 링크](https://huggingface.co/transformers/main_classes/tokenizer.html)\n",
    "\n",
    "> - Tokenize\n",
    "> - ![image-3.png](../../image/Pstage_CLUE_image1.png)\n",
    "\n",
    "> - BERT dataset 구축 순서\n",
    "    - 1) document 단위로 list append\n",
    "        - 하나의 document는 여러 개 문장으로 구성\n",
    "        - 하나의 document 구성 예시: [[1, 2, 3], [1, 3, 4]]\n",
    "    - 2) 1) list로부터 segmentA 구성\n",
    "        - documents를 iteration하면서 하나의 document를 선택\n",
    "        - target_seq_length: (max_num_tokens or random.randint(2, max_num_tokens))\n",
    "        - target_seq_length 보다 긴 경우 random하게 문장 단위로 자른다.\n",
    "            - 하나의 document안에 10개 문장이 있는 경우 ex) 1-5 / 1-7 / 1-3까지의 문장\n",
    "    - 3) segmentB 구성\n",
    "        - 다른 document or segmentA의 다음 문장\n",
    "        - **`Question`** : segmentB에 다른 document를 넣는 이유는?\n",
    "            - segmentA 이후 segmentB가 다음 문장인지 예측을 위해?\n",
    "    - 4) DataCollatorForLanguageModeling library를 이용해 `[MASK]` token 씌워주기\n",
    "\n",
    "### 시도한 것\n",
    "\n",
    "> - **`baseline`**에서 model, tokenizer를 **`monologg/koelectra-base-v3-discriminator`**로 변경\n",
    "> - **`max_length`** 100에서 200으로 변경\n",
    "    - 이유: entity가 100번째 token보다 뒤에 있을 가능성이 있기 때문에 (가정) 검증 X\n",
    "    - 69% -> 72%\n",
    "> - train, validation dataset split (9:1)\n",
    "    - data수가 1개인 label이 존재\n",
    "    - 해당 data를 제외한 뒤 split 후 train에 해당 data추가\n",
    "    - ![image-3.png](../../image/Pstage_CLUE_image2.png)\n",
    "\n",
    "### TODO\n",
    "\n",
    "> - Tokenizer 학습\n",
    "> - [SEP] token 추가\n",
    "\n",
    "> - wandb\n",
    "> - 제출 파일 엑셀 정리\n",
    "> - 전처리"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
