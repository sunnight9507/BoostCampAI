{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "handmade-arctic",
   "metadata": {},
   "source": [
    "## 1. Problems with deeper layers\n",
    "\n",
    "### 1.1 Going deeper with convolutions\n",
    "\n",
    "> - Deeper networks는 많은 feature를 학습할 수 있다.\n",
    "> - 그러나 깊어질 수록 gradient vanishing, exploding으로 학습이 잘 되지 않는다.\n",
    "> - 추가로 계산 복잡도가 늘어나게 된다.\n",
    "\n",
    "## 2. CNN architectures for image classfication 2\n",
    "\n",
    "### 2.1 GoogLeNet\n",
    "\n",
    "> - Inception module\n",
    "    - 하나의 layer에서 다양한 크기의 convolution filter를 사용\n",
    "    - 결과를 채널 축으로 concatenate하는 방식\n",
    "    - 1x1 convolution으로 채널 수를 줄여준다.\n",
    "    \n",
    "> - Overall architecture\n",
    "    - Stem network : 일반적인 vanila convolution networks\n",
    "    - Stacked inception modules\n",
    "    - Auxiliary classifiers\n",
    "    - Classifier output(a single FC layer)\n",
    "    \n",
    "> - Auxiliary classifier\n",
    "    - gradient vanishing 문제를 보안하기 위해 사용\n",
    "    - train 할 때만 사용\n",
    "    \n",
    "### 2.2 ResNet\n",
    "\n",
    "> - Deeper ResNets have lower error late\n",
    "> - layer가 깊어질수록 학습이 잘 되지 않는다.\n",
    "    - overfitting 문제는 아니다.\n",
    "    - degradation, optimization이 문제이다.\n",
    "    \n",
    "> - Shortcut connection or Skip connection\n",
    "    - gradient vanishing을 해결할 수 있다.\n",
    "\n",
    "> - Overall architecture\n",
    "    - He initalization / Additional conv layer at the beginning\n",
    "    - Stack residual blocks / 3x3 conv layers\n",
    "    - block을 넘어갈 때마다 공간을 반으로 줄이고 채널수는 2배로 늘어나게 했다.\n",
    "    - FC layer로 output 출력\n",
    "    \n",
    "### 2.3 Beyond ResNets\n",
    "\n",
    "> - DenseNet\n",
    "    - channel 축으로 concatenation\n",
    "    \n",
    "> - SENet\n",
    "    - Attention across channels\n",
    "    - Squeeze : global average pooling을 통해 각 채널의 공간 정보를 없애고 각 채널의 분포를 구한다.\n",
    "    - Excitation : FC layer를 통해 채널간의 연관성을 고려해서 attention score를 생성\n",
    "    \n",
    "> - Efficient Net\n",
    "    - compound scaling : Building deep, wide and high resolution networks in an efficient way\n",
    "    \n",
    "> - Deformable convolution\n",
    "    - 정사각형의 receptive field가 아닌 다양한 모양의 receptive field를 사용\n",
    "    \n",
    "## 3. Summary of image classification\n",
    "\n",
    "### 3.1 Summary of image classification\n",
    "\n",
    "> - AlexNet : simple CNN architecture\n",
    "    - Simple computation, but heavy memory size\n",
    "    - Low accuracy\n",
    "    \n",
    "> - VGGNet : simple with 3x3 convolution\n",
    "    - Highest memory, the heaviest computation\n",
    "\n",
    "> - GoogLeNet : inception module and auxiliary classifier\n",
    "\n",
    "> - ResNet : deeper layers with residual blocks\n",
    "    - Moderate efficiency (depending on the model)\n",
    "    \n",
    "### 3.2 CNN backbones\n",
    "\n",
    "> - GoogLeNet은 AlexNet, VGG, ResNet보다는 효율적이라고 볼 수 있다.\n",
    "    - 사용이 복잡하다.\n",
    "> - 대신에 VGGNet, ResNet으로 사용을 한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
