{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acquired-florist",
   "metadata": {},
   "source": [
    "### Transformer (cont'd)\n",
    "\n",
    "#### Transformer: Multi-Head Attention\n",
    "\n",
    "> - The input word vectors are the queries, keys and values\n",
    "> - In other words, the word vectors themselves select each other\n",
    "> - **Problem of single attention**\n",
    "    - Only one way for words to interact with one another\n",
    "> - **Solution**\n",
    "    - Multi-head attention maps $Q, K, V$ into the $h$ number of lower-dimensional spaces via $W$ matrices\n",
    "> - hen apply attention, then concatenate outputs and pipe through linear layer \n",
    "> - $\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat(head_1, \\dots, head_h)} W^O$\n",
    "> - $\\mathrm{where \\ head_i} = \\mathrm{Attention(QW_i^Q, KW_i^K, VW_i^V)}$\n",
    "\n",
    "> - Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types\n",
    "> - $n$ is the sequence length\n",
    "> - $d$ is the dimension of representation\n",
    "> - $k$ is the kernel size of convolutions\n",
    "> - $r$ is the size of the neighborhood in restricted self-attention\n",
    "> - | Layer Type | Complexity per Layer | Sequential Operations | Maximum Path Lenght |\n",
    "| :--- | :---: | :---: | :---: |\n",
    "| **Self-Attention** | $O(n^2 \\cdot d)$ | $O(1)$ | $O(1)$ |\n",
    "| **Recurrent** | $O(n \\cdot d^2)$ | $O(n)$ | $O(n)$ |\n",
    "| **Convolutional** | $O(k \\cdot n \\cdot d^2)$ | $O(1)$ | $O(log_k(n))$ |\n",
    "| **Self-Attention (restricted)** | $O(r \\cdot n \\cdot d)$ | $O(1)$ | $O(n/r)$ |\n",
    "\n",
    "#### Transformer: Block-Based Model\n",
    "\n",
    "> - Each block has two sub-layers\n",
    "    - Multi-head attention\n",
    "    - Two-layer feed-forward NN (with ReLU)\n",
    "> - Each of these two steps also has\n",
    "    - Residual connection and layer normalization\n",
    "    - $\\mathrm{LayerNorm}(x + \\mathrm{sublayer}(x))$\n",
    "    \n",
    "#### Transformer: Layer Normalization\n",
    "\n",
    "> - Layer normalization changes input to have zero mean and unit variance, per layer and per training point (and adds two more parameters)\n",
    "    - ex) Batch, Layer, Instance, Group Norm\n",
    "> - $u^l = \\frac{1}{H} \\sum_{i=1}^H a_i^l$\n",
    "> - $\\sigma^l = \\sqrt{\\frac{1}{H} \\sum_{i=1}^H (a_i^l - \\mu^l)^2}$\n",
    "> - $h_i = f(\\frac{g_i}{\\sigma_i}(a_i - \\mu_i) + b_i)$\n",
    "\n",
    "> - **Layer normalization consists of two steps**\n",
    "    - Normalization of each word vectors to have mean of zero and variance of one\n",
    "    - Affine transformation of each sequence vector with learnable parameters\n",
    "    \n",
    "#### Transformer: Positional Encoding\n",
    "\n",
    "> - Use sinusoidal functions of different frequencies\n",
    "> - $\\mathrm{PE}_{(pos, 2i)} = \\sin{(pos / 10000^{2i / d_{\\mathrm{model}}})}$\n",
    "> - $\\mathrm{PE}_{(pos, 2i+1)} = \\cos{(pos / 10000^{2i / d_{\\mathrm{model}}})}$\n",
    "> - Easily learn to attend by relative position, since for any fixed offset $k, \\mathrm{PE}_{(pos+k)}$ can be represented as linear function of $\\mathrm{PE}_{(pos)}$\n",
    "\n",
    "#### Transformer: Decoder\n",
    "\n",
    "> - Two sub-layer changes in decoder\n",
    "> - Masked decoder self-attention on previously generated outputs\n",
    "> - Encoder-Decoder attention, where queries come from previous decoder layer and keys and values come from output of encoder\n",
    "\n",
    "#### Transformer: Masked Self-Attention\n",
    "\n",
    "> - Those words not yet generated cannot be accessed during the inference time\n",
    "> - Renormalization of softmax output prevents the model from accessing ungenerated words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
