{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "developmental-second",
   "metadata": {},
   "source": [
    "### 1. Closed-book Question Answering\n",
    "\n",
    "#### Idea of Closed-book Question Answering\n",
    "\n",
    "- 모델이 이미 사전학습으로 대량의 지식을 학습했다면, 사전학습 언어모델 자체가 이미 하나의 knowledge storage라고 볼 수 있지 않을까? => 굳이 다른 곳에서 지식을 가져와야할 필요가 없지 않을까?\n",
    "\n",
    "#### Zero-shot QA performance of GPT-2\n",
    "\n",
    "- 사전학습 시 전혀 본 적없는 Natural Questions 데이터셋에도 어느 정도 대답이 가능함\n",
    "\n",
    "#### Open-book QA vs. Closed-book QA\n",
    "\n",
    "- 문제점\n",
    "    - Open-book QA : 지식 소스를 저장하기 어려움, 검색하는 시간 소요\n",
    "    - Closed-book QA : 사전학습된 언어 모델이 얼마나 지식을 잘 기억하고 있는지가 매우 중요함\n",
    "    \n",
    "### 2. Text-to-Text Format\n",
    "\n",
    "#### Closed-book QA as Text-to-Text Format\n",
    "\n",
    "- Generation-based MRC와 유사\n",
    "    - 입력에 지문(Context)가 없이 질문만 들어간다는 것이 차이점\n",
    "    - 사전학습된 언어 모델은 BART와 같은 seq-to-seq 형태의 Transformer 모델을 사용함\n",
    "    - Text-to-Text format에서는 각 입력값(질문)과 출력값(답변)에 대한 설명을 맨 앞에 추가함\n",
    "    \n",
    "#### Text-to-Text Format\n",
    "\n",
    "- input으로 text를 받아서, output으로 새로운 text를 생성하는 문제\n",
    "- 다양한 text processing problem => Text-to-text 문제로 변형\n",
    "\n",
    "#### T5\n",
    "\n",
    "- Text-to-Text format이라는 형태로 데이터의 입출력을 만들어 거의 모든 자연어처리 문제를 해결하도록 학습된 seq-to-seq 형태의 Transformer 모델\n",
    "\n",
    "### Experiment Results & Analysis\n",
    "\n",
    "#### Quantitative Examples\n",
    "\n",
    "- 대부분의 Open-book 스타일 모델 (문서 검색 후 기계 독해)보다 뛰어난 성능을 보여줌\n",
    "- 모델 크기가 커질수록 성능이 증가함\n",
    "- Salient Span Masking이 성능을 크게 끌어올림\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Closed-book QA의 한계점 및 앞으로의 개선 방향\n",
    "    - 모델의 크기가 커서 계산량이 많고 속도가 느림 => 더 효율적인 모델 필요\n",
    "    - 모델이 어떤 데이터로 답을 내는지 알 수 없음 => 결과의 해석 가능성(interpretability)을 높이는 연구 필요\n",
    "    - 모델이 참조하는 지식을 추가하거나 제거하기 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-poker",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### 시도한 것\n",
    "\n",
    "- Reader\n",
    "    - Data preprocessing\n",
    "        - 필요 없는 특수 문자 제거 ex) \\n, \\s+ ... => EM 기준 약1.5% 상승 & F1 score 기준 2% 상승\n",
    "    - Data augmentation\n",
    "        - Question에서 단어만 확률적으로 MASK 20% 확률 => EM 기준 약1.5% 상승 & F1 score 기준 약 10% 상승!\n",
    "\n",
    "### 새로 알게된 것\n",
    "\n",
    "### TODO\n",
    "\n",
    "- **Retireval**\n",
    "    - Doc2Vec\n",
    "    - Sentence transformers\n",
    "    - Ensemble\n",
    "    \n",
    "\n",
    "- **Reader**\n",
    "    - Span masking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
