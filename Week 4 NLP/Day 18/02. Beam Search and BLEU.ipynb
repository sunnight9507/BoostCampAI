{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "naughty-rates",
   "metadata": {},
   "source": [
    "### Beam search\n",
    "\n",
    "#### Greedy decoding\n",
    "\n",
    "> - Greedy decoding has no way to undo decisions\n",
    "\n",
    "#### Exhaustive search\n",
    "\n",
    "> - Ideally, we want to find a (length $T$) translation $y$ that maximizes\n",
    "> - $P(y|x) = P(y_1|x) P(y_2|y_1, x) \\dots P(y_T|y_1,\\dots,y_{T-1}, x) = \\prod_1^T P(y_t|y_1, \\dots, y_{t-1}, x) $\n",
    "\n",
    "> - We could try computing **all possible sequences** $y$\n",
    "    - This means that on each step $t$ of the decoder, we are tracking $V^t$ possible partial translations, where $V$ is the vocabulary size\n",
    "    - This $O(V_t)$ complexity is far too expensive\n",
    "    \n",
    "#### Beam search\n",
    "\n",
    "> - Core idea: on each time step of the decoder, we keep track of the $k$ most probable partial translations (which we call hypothese)\n",
    "    - $k$ is the beam size (in practice around 5 to 10)\n",
    "    \n",
    "> - A hypothesis $y_1,\\dots,y_t$ has a score of its log probability:\n",
    "    - $\\mathrm{score}(y_1, \\dots, y_t) = \\log{P_{LM}}(y_1, \\dots, y_t | x) = \\sum_{i=1}^t \\log{P_{LM}}(y_i|y_1, \\dots, y_{i-1}, x)$\n",
    "    - Scores are all negative, and a higher score is better\n",
    "    - We search for high-scoring hypotheses, tracking the top $k$ ones on each step\n",
    "    \n",
    "> - Beam search is **not guaranteed** to find a globally optimal solution. \n",
    "> - But it is **much more efficient** than exhaustive search!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-yemen",
   "metadata": {},
   "source": [
    "> #### Beam search: Stopping criterion\n",
    "> - In **`greedy decoding`**, usually we decode until the model produces a **<END> token**\n",
    "    - For example: <START> he hit me with a pie <END>\n",
    "> - In **`beam search decoding`**, different hypotheses may produce <END> tokens on **different timesteps**\n",
    "    - When a hypothesis produces <END>, that hypothesis is **complete**\n",
    "    - **Place it aside** and continue exploring other hypotheses via beam search\n",
    "> - Usually we continue beam search until:\n",
    "    - We reach timestep $T$ (where $T$ is some pre-defined cutoff), or\n",
    "    - We have at least $n$ completed hypotheses (where $n$ is the pre-defined cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-seeker",
   "metadata": {},
   "source": [
    "> #### Beam search: Finishing up\n",
    "> - We have our list of completed hypotheses\n",
    "> - How to select the top one with the highest score?\n",
    "> - Each hypothesis $y_1, \\dots, y_t$ on our list has a score\n",
    "    - $\\mathrm{score}(y_1, \\dots, y_t) = \\log{P_{LM}}(y_1, \\dots, y_t | x) = \\sum_{i=1}^t \\log{P_{LM}}(y_i|y_1, \\dots, y_{i-1}, x)$\n",
    "> - Problem with this : **longer hypotheses have lower scores**\n",
    "> - Fix: Normalize by length\n",
    "    - $\\mathrm{score}(y_1, \\dots, y_t) = \\frac{1}{t} \\sum_{i=1}^t \\log{P_{LM}}(y_i|y_1, \\dots, y_{i-1}, x)$\n",
    "\n",
    "### BLEU score\n",
    "\n",
    "> #### Precision and Recall\n",
    "> - Reference: **Half** of **my heart is in** Havana **ooh na** na\n",
    "> - Predicted: **Half** as **my heart is in** Obama **ooh na**\n",
    "> - $ precision = \\frac{\\#(correct words)}{length \\ of \\ prediction} = \\frac{7}{9} = 78%$\n",
    "> - $ recall = \\frac{\\#(correct words)}{length \\ of \\ reference} = \\frac{7}{10} = 70%$\n",
    "> - $ F-measure = \\frac{precision \\ * \\ recall}{\\frac{1}{2}(precision + recall)} = \\frac{0.78*0.7}{0.5*(0.78+0.7)} = 73.78%$\n",
    "> - **Flaw: no penalty for reordering**\n",
    "\n",
    "> #### BiLingual Evaluation Understudy (BLEU) \n",
    "> - **N-gram overlap** between machine translation output and reference sentence\n",
    "> - Compute **precision for n-grams of size one to four**\n",
    "> - Add brevity penalty (for too short translations)\n",
    "    - $BLUE = \\min{(1, \\frac{length \\ of \\ prediction}{length \\ of \\ reference}}) (\\prod_{i=1}^4 precision_i)^\\frac{1}{4}$\n",
    "    - Typically computed over the entire corpus, not on single sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
