{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infectious-health",
   "metadata": {},
   "source": [
    "#### ILSVRC\n",
    "\n",
    "> - **I**mageNet **L**arge-**S**cale **V**isual **R**ecognition **C**hallenge\n",
    "    - Classification / Detection / Localization / Segmentation\n",
    "    - 1,000 different categories\n",
    "    - Over 1 million images\n",
    "    - Training set : 456,567\n",
    "    \n",
    "#### AlexNet\n",
    "\n",
    "> - gpu를 활용하기 위해 network를 2개로 나눔\n",
    "> - Learned 11x11x3 filters\n",
    "> - 5 convolutional layers\n",
    "> - 3 dense layers\n",
    "\n",
    "> #### Key ideas\n",
    "> - ReLU(Rectified Linear Unit) 활성함수 사용\n",
    "> - GPU implementation (2 GPUs)\n",
    "> - Data augmentation\n",
    "> - Dropout\n",
    "\n",
    "> #### ReLU Activation\n",
    "> - 선형 모델이 가지고 있는 성질(grad를 그래도 보존)\n",
    "> - Easy to optimize with gradient descent\n",
    "> - Good generalization\n",
    "> - Overcome the vanishing gradient problem\n",
    "\n",
    "#### VGGNet\n",
    "\n",
    "> - 3x3 convolution filter만 사용(stride 1)\n",
    "> - 1x1 convolution for fully connected layers\n",
    "> - Dropout(p=0.5)\n",
    "> - VGG16, VGG19\n",
    "\n",
    "> - Why 3x3 convolution?\n",
    "> - Receptive field : layer의 값 하나에 영향을 미치는 input\n",
    "layer의 공간 크기\n",
    "    - 3x3을 두번 하는것은 5x5하는 것과 Receptive field 측면에서 동일\n",
    "> - parameter 수\n",
    "    - 3x3 2번 : 3x3x128x128 + 3x3x128x128 = 294,912\n",
    "    - 5x5 : 5x5x128x128 = 409,600\n",
    "    \n",
    "#### GoogLeNet\n",
    "\n",
    "> - It conbined network-in-network(NiN) with inception blocks\n",
    "\n",
    "> #### Inception blocks\n",
    "> - Reduce the number of parameter\n",
    "> - 1x1 convolution으로 채널수를 줄여 parameter 수를 줄인다.\n",
    "    - 3x3 convolution : 3x3x128x128 = 147,456\n",
    "    - 1x1 -> 3x3 : 1x1x128x32 + 3x3x32x128 = 38,864\n",
    "    \n",
    "#### ResNet\n",
    "\n",
    "> - Network가 깊어져도 학습이 잘 되지 않는 문제\n",
    "    - 56 layer가 20 layer 보다 training, test의 결과가 좋지 못함\n",
    "> - identity map(skip connection)을 추가\n",
    "    - $f(x) \\rightarrow x + f(x)$\n",
    "    - network를 딥하게 쌓을 수 있는 가능성을 보여줌\n",
    "> - Batch normalization after convolutions\n",
    "> - Bottleneck architecture\n",
    "> - **Performance** increases while **parameter size** decreases\n",
    "\n",
    "#### DenseNet\n",
    "\n",
    "> - DenseNet uses concatenation instead of addition\n",
    "\n",
    "#### Key takeaways\n",
    "\n",
    "> - VGG : repeated 3x3 blocks\n",
    "> - GoogLeNet : 1x1 convolution\n",
    "> - ResNet : skip-connection\n",
    "> - DenseNet : concatenation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
