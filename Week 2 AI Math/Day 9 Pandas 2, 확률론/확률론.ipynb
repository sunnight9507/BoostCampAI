{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deluxe-literacy",
   "metadata": {},
   "source": [
    "#### 딥러닝에서 확률론이 왜 필요한지?\n",
    "\n",
    "> - 딥러닝은 **확률론 기반의 기계학습 이론**에 바탕을 두고 있다.\n",
    "> - 기계학습에서 사용되는 손실함수(loss function)들의 작동 원리는 데이터 공간을 통계적으로 해석해서 유도하게 된다.\n",
    "> - 회귀 분석에서 손실함수로 사용되는 $L_2$ Norm은 **예측오차의 분산을 가장 최소화하는 방향으로 학습**하도록 유도한다.\n",
    "> - 분류 문제에서 사용되는 교차엔트로피(cross-entropy)는 **모델 예측의 불확실성을 최소화하는 방향으로 학습**하도록 유도한다.\n",
    "> - 분산 및 불확실성을 **최소화하기 위해서는 측정하는 방법**을 알아야 한다.\n",
    "\n",
    "#### 확률분포는 데이터의 초상화\n",
    "\n",
    "> - 데이터 공간을 $ \\mathcal{X} $ X $ \\mathcal{Y} $라 표기하고 $ \\mathcal{D} $는 데이터공간에서 데이터를 추출하는 분포이다.\n",
    "> - 데이터는 확률변수로 $(\\mathbf{x}, y) \\thicksim \\mathcal{D}$라 표기\n",
    "    - $(\\mathbf{x}, y) \\in \\mathcal{X} $ X $ \\mathcal{Y}$는 데이터공간 상의 **관측가능한 데이터**에 해당한다.\n",
    "> - 결합분포 $(\\mathbf{x}, y)$는 $\\mathcal{D}$를 모델링한다.\n",
    "    - $\\mathcal{D}$는 이론적으로 존재하는 확률분포이기 때문에 사전에 알 수 없다.\n",
    "> - $P(X = \\mathbf{x})$는 입력 $\\mathbf{x}$에 대한 주변확률분포로 $y$에 대한 정보를 주진 않는다.\n",
    "    - $P(X = \\mathbf{x}) = \\sum_y P(\\mathbf{x}, y)$\n",
    "    - $P(X = \\mathbf{x}) = \\int_y P(\\mathbf{x}, y) dy$\n",
    "    - 주변확률분포 $P(\\mathbf{x})$는 결합분포 $P(\\mathbf{x}, y)$에서 유도 가능하다.\n",
    "> - 조건부확률분포 $P(\\mathbf{x}|y)$는 데이터 공간에서 입력 $\\mathbf{x}$와 출력 $y$사이의 관계를 모델링한다.\n",
    "    - $P(\\mathbf{x}|y)$는 특정 클래스가 주어진 조건에서 데이터의 확률분포를 보여준다.\n",
    "    \n",
    "#### 이산확률변수 vs 연속확률변수\n",
    "\n",
    "> - 확률변수는 확률분포 $\\mathcal{D}$에 따라 **이산형(discrete)**과 **연속형(continuous)**로 구분하게 된다.\n",
    "    - 데이터공간 $\\mathcal{X} $ X $ \\mathcal{Y}$에 의해 결정되는 것이 아니라 $\\mathcal{D}$에 의해 결정된다.\n",
    "> - 이산형 확률변수는 **확률변수가 가질 수 있는 경우의 수**를 모두 고려하여 **확률을 더해서 모델링**한다.\n",
    "> - $\\mathbb{P}(X \\in A) = \\sum_{\\mathbf{x} \\in A} P(X = \\mathbf{x})$\n",
    "    - $P(X = \\mathbf{x})$는 확률변수가 $\\mathbf{x}$값을 가질 확률로 해석할 수 있다.\n",
    "> - 연속형 확률변수는 **데이터 공간에 정의된 확률변수의 밀도(density)**위에서의 **적분을 통해 모델링**한다.\n",
    "> - $\\mathbb{P}(X \\in A) = \\int_A P(\\mathbf{x})d\\mathbf{x}$\n",
    "    - $P(\\mathbf{x}) = \\lim_{h \\to 0} \\frac{\\mathbb{P}(\\mathbf{x} - h \\le X \\le \\mathbf{x} + h)}{2h} $\n",
    "    - 밀도는 누적확률분포의 변화율을 모델링하며 확률로 해석하면 안된다.\n",
    "    \n",
    "#### 조건부확률과 기계학습\n",
    "\n",
    "> - 조건부확률 $P(y|\\mathbf{x})$는 입력변수 $\\mathbf{x}$에 대해 정답이 $y$일 확률을 의미한다.\n",
    "    - 연속확률분포의 경우 $P(y|\\mathbf{x})$는 확률이 아니고 밀도로 해석한다는 것을 주의해야 한다.\n",
    "> - 로지스틱 회귀에서 사용했던 선형모델과 소프트맥스 함수의 결합은 **데이터에서 추출된 패턴을 기반으로 확률을 해석**하는데 사용된다.\n",
    "> - 분류 문제에서 softmax($\\mathbf{W}\\phi + \\mathbf{b}$)은 데이터 $\\mathbf{x}$로부터 추출된 특징패턴 $\\phi(\\mathbf{x})$과 가중치행렬 $\\mathbf{W}$을 통해 조건부확률 $P(y|\\mathbf{x})$을 계산한다.\n",
    "    - $P(y|\\phi(\\mathbf{x}))$라 써도 된다.\n",
    "> - 회귀 문제의 경우 조건부기대값 $\\mathbb{E}[y | \\mathbf{x}]$을 추정한다.\n",
    "    - 조건부기대값은 $\\mathbb{E} \\Vert y - f(\\mathbf{x}) \\Vert_2$을 최소화하는 함수 $f(\\mathbf{x})$와 일치한다.\n",
    "> - 딥러닝은 다층신경망을 사용하여 데이터로부터 특징패턴$(\\phi)$을 추출한다.\n",
    "    - 특징패턴을 학습하기 위해 어떤 손실함수를 사용할지는 기계학습 문제와 모델에 의해 결정된다.\n",
    "    \n",
    "#### 기대값\n",
    "\n",
    "> - 확률분포가 주어지면 데이터를 분석하는 데 사용 가능한 여러 종류의 **통계적 범함수를 계산**할 수 있다.\n",
    "> - **기대값은 데이터를 대표하는 통계량**이면서 동시에 확률분포를 통해 다른 통계적 범함수를 계산하는데 사용된다.\n",
    "> - 기대값을 이용해 분산, 첨도, 공분산 등 여러 통계량을 계산할 수 있다.\n",
    "\n",
    "#### 몬테카를로 샘플링\n",
    "\n",
    "> - 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 대부분이다.\n",
    "> - 확률분포를 모를 때 **데이터를 이용하여 기대값을 계산하려면 몬테카를로 샘플링**방법을 사용 해야 한다.\n",
    "    - 몬테카를로는 이산형이든 연속형이든 상관없이 성립한다.\n",
    "> - 몬테카를로 샘플링은 독립추출만 보장한다면 **대수의 법칙**에 의해 수렴성을 보장한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
