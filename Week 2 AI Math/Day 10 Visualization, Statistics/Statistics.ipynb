{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dominican-anger",
   "metadata": {},
   "source": [
    "#### 모수\n",
    "\n",
    "> - **통계적 모델링은 적절한 가정 위에서 확률분포를 추정(inference)**하는 것이 목표이며, 기계학습과 통계학이 공통적으로 추구하는 목표\n",
    "> - 그러나 유한한 개수의 데이터만 관찰해서 모집단의 분포를 정확하게 알아낸다는 것은 불가능하므로, **근사적으로 확률분포를 추정**할 수 밖에 없습니다.\n",
    "> - 데이터가 특정 확률분포를 따른다고 선험적으로(a priori) 가정한 후 그 분포를 결정하는 모수(parameter)를 추정하는 방법을 **모수적(parametric) 방법론**이라 한다.\n",
    "> - 특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌면 **비모수(nonparametric) 방법론**이라 부른다.\n",
    "    - 기계학습의 많은 방법론은 비모수 방법론에 속한다.\n",
    "    \n",
    "#### 확률분포 종류\n",
    "\n",
    "> - 베르누이 : 데이터가 2개의 값(0, 1)으로만 이루어진 경우\n",
    "> - 카테고리 : n개의 이산적인 값을 가진 경우\n",
    "> - 베타 : [0, 1] 사이에서 값을 가지는 경우\n",
    "> - 감마, 로그 정규분포 : 0이상의 값을 가지는 경우\n",
    "> - 정규, 라플라스분포 : 데이터가 $\\mathbb{R}$ 전체에서 값을 가지는 경우\n",
    "> - 기계적으로 확률분포를 가정해서는 안 되며, **데이터를 생성하는 원리를 먼저 고려하는 것이 원칙**이다.\n",
    "    - 각 분포마다 검정하는 방법들이 있으므로 모수를 추정한 후에는 반드시 검정을 해야 한다.\n",
    "    \n",
    "#### 모수 추정\n",
    "\n",
    "> - 데이터의 확률분포를 가정했다면 모수를 추정해볼 수 있다.\n",
    "> - 정규분포의 모수는 평균$\\mu$과 분산 $\\sigma^2$이고 다음과 같이 표현할 수 있다.\n",
    "    - **표본 평균**\n",
    "    - $\\bar{X} = \\frac{1}{N}\\sum_{i=1}^N X_i$\n",
    "    - $\\mathbb{E}[\\bar{X}] = \\mu$\n",
    "    -\n",
    "    - **표본 분산**\n",
    "    - $S^2 = \\frac{1}{N-1} \\sum_{i=1}^{N} (X_i - \\bar{X})^2$\n",
    "    - $\\mathbb{E}[S^2] = \\sigma^2$\n",
    "    - 표본분산을 구할 때 $N - 1$로 나누는 이유는 불편 추정량을 구하기 위해서이다.\n",
    "> - **통계량의 확률분포를 표집분포(sampling distribution)**이라 하며, 특히 표본평균의 표집분포는 $N$이 커질수록 정규분포 $N(\\mu, \\frac{\\sigma^2}{N})$을 따른다.\n",
    "    - 이를 **중심극한정리**라 하며, 모집단의 분포가 정규분포를 따르지 않아도 성립한다.\n",
    "    \n",
    "#### 최대가능도 추정법\n",
    "\n",
    "> - 표본평균이나 표본분산은 중요한 통계량이지만 확률분포마다 사용하는 모수가 다르므로 적절한 통계량은 달라지게 된다.\n",
    "> - 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나는 **최대가능도 추정법(maximun likelihood estimation, MLE)**이다.\n",
    "> - 가능도 함수는 모수 $\\theta$를 따르는 분포가 $\\mathbf{x}$를 관찰할 가능성을 뜻하지만 확률로 해석하면 안된다.\n",
    "> - 데이터 집합 $X$\n",
    "> - $ \\hat{\\theta}_{MLE} = \\underset{\\theta}{\\mathrm{argmax}} L(\\theta; \\mathbf{x}) = \\underset{\\theta}{\\mathrm{argmax}} \n",
    "L(\\mathbf{x}|\\theta)$\n",
    "> - 데이터 집합 $X$가 **독립적으로 추출되었을 경우 로그가능도를 최적화**한다.\n",
    "    - $L(\\theta; \\mathbf{x}) = \\prod_{i=1}^N P(\\mathbf{x}_i|\\theta) \\Rightarrow \\log L(\\theta; \\mathbf{x}) = \\sum_{i=1}^{n} \\log P(\\mathbf{x}_i|\\theta)$\n",
    "    \n",
    "#### 로그가능도\n",
    "\n",
    "> - 로그가능도를 최적화하는 모수 $\\theta$는 가능도를 최적화하는 MLE가 된다.\n",
    "> - 데이터의 숫자가 적으면 상관없지만 **만일 데이터의 숫자가 수억 단위가 된다면 컴퓨터의 정확도로는 가능도를 계산하는 것은 불가능**하다.\n",
    "> - 데이터가 독립일 경우, 로그를 사용하면 가능도의 곱셈을 로그가능도의 덧셈으로 바꿀 수 있기 때문에 컴퓨터로 연산이 가능해진다.\n",
    "> - 경사하강법으로 가능도를 최적화할 때 미분 연산을  사용하게 되는데, 로그가능도를 사용하면 **연산량을 $O(n^2)$에서 $O(n)$으로 줄여준다.**\n",
    "> - 대게의 손실함수의 경우 경사하강법을 사용하므로 **음의 로그가능도(negative log-likelihood)를 최적화**하게 된다.\n",
    "\n",
    "#### 확률분포의 거리\n",
    "\n",
    "> - 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰된느 확률분포의 거리를 통해 유도한다.\n",
    "> - 데이터공간에 두 개의 확률분포 $P(\\mathbf{x}), Q(\\mathbf{x})$가 있을 경우 **두 확률분포 사이의 거리(distance)**를 계산할 때 다음과 같은 함수들을 이용한다.\n",
    "    - 총변동 거리(Total Variation Distance, TV)\n",
    "    - 쿨백-라이블러 발산(Kullback-Leibler Divergence, KL)\n",
    "    - 바슈타인 거리(Wasserstein Distance)\n",
    "    \n",
    "#### 쿨백-라이블러 발산\n",
    "\n",
    "> - 이산확률변수일 때\n",
    "> - $\\mathbb{KL}(P||Q) = \\sum_{\\mathbf{x} \\in \\mathcal{X}} P(\\mathbf{x}) \\log(\\frac{P(\\mathbf{x})}{Q(\\mathbf{x})})$\n",
    "\n",
    "> - 연속확률변수일 때\n",
    "> - $\\mathbb{KL}(P||Q) = \\int_{\\mathcal{X}} P(\\mathbf{x}) \\log(\\frac{P(\\mathbf{x})}{Q(\\mathbf{x})}) d\\mathbf{x}$\n",
    "\n",
    "> - 쿨백 라이블러는 다음과 같이 분해할 수 있다.\n",
    "> - $\\mathbb{KL}(P||Q) =  \\underset{\\text{크로스엔트로피}}{- \\mathbb{E}_{\\mathbf{x} \\thicksim P(\\mathbf{x})}\\begin{bmatrix} \\log Q(\\mathbf{x}) \\end{bmatrix}} + \\underset{\\text{엔트로피}}{\\mathbb{E}_{\\mathbf{x} \\thicksim P(\\mathbf{x})}\\begin{bmatrix} \\log P(\\mathbf{x}) \\end{bmatrix}}$\n",
    "> - 분류 문제에서 정답레이블을 $P$, 모델 예측을 $Q$라 두면 **최대가능도 추정법은 쿨백-라이블러 발산을 최소화**하는 것과 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
