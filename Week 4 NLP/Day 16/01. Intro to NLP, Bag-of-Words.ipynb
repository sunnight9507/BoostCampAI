{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "departmental-therapy",
   "metadata": {},
   "source": [
    "#### Natural Language Processing\n",
    "\n",
    "> - 주요 학회 : ACL, EMNLP, NAACL\n",
    "> - Low-level parsing\n",
    "    - Tokenization, stemming\n",
    "> - Word and phrase level\n",
    "    - NER(Named entity recognition), POS(part of speech) tagging\n",
    "> - Sentence level\n",
    "    - Sentiment analysis, Machine translation\n",
    "> - Multi-sentence and paragraph level\n",
    "    - Entailment prediction(논리 여부 확인), Question answering,  Dialog systems, Summarization\n",
    "    \n",
    "#### Text Mining\n",
    "\n",
    "> - 주요 학회 : KDD, The WebConf(formerly, WWW), WSDM, CIKM, ICWSM\n",
    "> - Extract useful information and insights from text and document data\n",
    "    - analyzing the trends of AI-related keywords from massive news data\n",
    "> - Document clustering\n",
    "    - topic modeling, grouping into different subjects\n",
    "> - Highly related to computational social science\n",
    "    - analyzing the evolution of people's political tendency based on social media data\n",
    "    \n",
    "#### Information retrieval\n",
    "\n",
    "> - 주요 학회 : SIGIR, WSDM, CIKM, RecSys\n",
    "> - This area is not actively studied now\n",
    "> - It has evolved into a recommendation system, which is still an active area of research\n",
    "\n",
    "#### Trends of NLP\n",
    "\n",
    "> - Text data can basically be viewed as a sequence of words, and **each word can be represented as a vector** through a technique such as Word2Vec or GloVe.\n",
    "> - **RNN-family models**(LSTMs and GRUs), which take the sequence of these vectors of words as input, are the main architecture of NLP tasks.\n",
    "> - Overall performance of NLP tasks has been improved since **attention modules and Transformer models**, which replaced RNNs with self-attention, have been introduced a few years ago.\n",
    "> - As is the case for Transformer models, most of the advanced NLP models have been originally developed for improving machine translation tasks.\n",
    "\n",
    "> - In the early days, **customized models for different NLP tasks** had developed separately.\n",
    "> - Since Transformer was introduced, huge models were released by stacking its basic module, self-attention, and these models are trained with large-sized datasets through language modeling tasks, one of the **self-supervised training setting that does not require additional labels** for a particular task.\n",
    "    - Ex) BERT, GPT-3\n",
    "> - Afterwards, above models were applied to other tasks through **transfer learning**, and they outperformed all other customized models in each task. \n",
    "> - Currently, these models has now become essential part in numerous NLP tasks, so **NLP research become difficult with limited GPU resources**, since they are too large to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-directory",
   "metadata": {},
   "source": [
    "### Bag-of-Words\n",
    "\n",
    "#### Bag-of-Words Representation\n",
    "\n",
    "> #### Step 1. Constructing the vocabulary containing unique words\n",
    "> - Example sentences: “John really really loves this movie“, “Jane really likes this song”\n",
    "> - Vocabulary: {“John“, “really“, “loves“, “this“, “movie“, “Jane“, “likes“, “song”}\n",
    "\n",
    "> #### Step 2. Encoding unique words to one-hot vectors\n",
    "> - Vocabulary: {“John“, “really“, “loves“, “this“, “movie“, “Jane“, “likes“, “song”}\n",
    "    - John: [1 0 0 0 0 0 0 0]\n",
    "    - really: [0 1 0 0 0 0 0 0]\n",
    "    - loves: [0 0 1 0 0 0 0 0]\n",
    "    - this: [0 0 0 1 0 0 0 0]\n",
    "    - movie: [0 0 0 0 1 0 0 0]\n",
    "> - For any pair of words, the distance is 2\n",
    "> - For any pair of words, cosine similarity is 0\n",
    "\n",
    "> #### A sentence/document can be represented as the sum of one-hot vectors\n",
    "> - Sentence 1: “John really really loves this movie“\n",
    "    - John + really + really + loves + this + movie: [1 2 1 1 1 0 0 0]\n",
    "> - Sentence 2: “Jane really likes this song”\n",
    "    - Jane + really + likes + this + song: [0 1 0 1 0 1 1 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-raising",
   "metadata": {},
   "source": [
    "#### NaiveBayes Classifier for Document Classification\n",
    "\n",
    "> - Bayes’ Rule Applied to Documents and Classes\n",
    "> - For a document $d$ and a class $c$\n",
    "> - $C_{\\mathrm{MAP}} = \\underset{c \\in C}{\\mathrm{argmax}} P(c | d) \\rightarrow$ MAP is “maximum a posteriori”  = most likely class\n",
    "> - $ = \\underset{c \\in C}{\\mathrm{argmax}} \\frac{P(d|c) P(c)}{P(d)} \\rightarrow$ Bayes Rule\n",
    "> - $ = \\underset{c \\in C}{\\mathrm{argmax}} P(d|c) P(c) \\rightarrow$ Dropping the denominator\n",
    "\n",
    "> - For a document $d$, which consists of a sequence of words $w$, and a class $c$\n",
    "> - The probability of a document can be represented by multiplying the probability of each word appearing\n",
    "> - $P(d|c)P(c) = P(w_1, w_2, \\dots, w_n|c)P(c) \\rightarrow p(c) \\prod_{w_i \\in W}P(w_i|c)$(by conditional independence assumption)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
