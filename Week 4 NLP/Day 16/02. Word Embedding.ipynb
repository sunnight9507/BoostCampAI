{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fewer-synthetic",
   "metadata": {},
   "source": [
    "#### Word Embedding\n",
    "\n",
    "> - Express a word as a vector\n",
    "> - 'cat' and 'kitty' are similar words, so they have similar vector representations → short distance\n",
    "> - 'hamburger' is not similar with 'cat' or 'kitty’,  so they have different vector representations → far distance\n",
    "\n",
    "#### Word2Vec\n",
    "\n",
    "> - An algorithm for training vector representation of a word from context words (adjacent words)\n",
    "> - Assumption: words in similar context will have similar meanings\n",
    "\n",
    "#### Idea of Word2Vec\n",
    "\n",
    "> - \"You shall know a word by the company it keeps\" – J.R. Firth 1957\n",
    "> - Suppose we read the word \"cat\"\n",
    "    - What is the probability $P(w|\\mathrm{cat})$ that we'll read the word $w$ nearby?\n",
    "> - Distributional Hypothesis: The meaning of \"cat\" is captured by the probability distribution $P(w|\\mathrm{cat})$\n",
    "\n",
    "#### How Word2Vec Algorithm Works\n",
    "\n",
    "> - Sentence : \"I study math.\"\n",
    "> - Vocabulary: {\"I\", \"study\", \"math\"}\n",
    "> - Input: \"study\" : [0, 1, 0]\n",
    "> - Output: \"math\" : [0, 0, 1] \n",
    "> - Columns of $W_1$ and rows of $W_2$ represent each word \n",
    "> - E.g., \"study\" vector :  2nd column in $W_1$, \"math\" vector : 3rd row in $W_2$.\n",
    "> - The \"study\" vector in $W_1$ and the \"math\" vector in $W_2$ should have a high inner-product value.\n",
    "\n",
    "#### Application of Word2Vec\n",
    "\n",
    "> #### Word2Vec improves performances in most areas of NLP\n",
    "> - Word similarity\n",
    "> - Machine translation\n",
    "> - Part-of-speech (PoS) tagging \n",
    "> - Named entity recognition (NER)\n",
    "> - Sentiment analysis\n",
    "> - Clustering \n",
    "> - Semantic lexicon building\n",
    "\n",
    "#### Glove (Global Vectors for Word Representation)\n",
    "\n",
    "> - Rather than going through each pair of an input and an output words, it first computes the co-occurrence matrix, to avoid training on identical word pairs repetitively. \n",
    "> - Afterwards, it performs matrix decomposition on this co-occurrent matrix. \n",
    "> - $J(\\theta) = \\frac{1}{2} \\sum_{i, j = 1}^{W} f(P_{ij})(u_i^T v_j - \\log{P_{ij}})^2$\n",
    "> - Fast training\n",
    "> - Works well even with a small corpus\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
