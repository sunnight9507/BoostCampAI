{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "literary-condition",
   "metadata": {},
   "source": [
    "#### Transformer\n",
    "\n",
    "> - **Transformer** is the first sequence transduction model based entirely on attention \n",
    "> - input sequence, output sequence의 길이, 도메인이 다를 수 있다.\n",
    "> - Six identical(but not shared) **encoders** and **decoders** are stacked\n",
    "> - input으로 하나의 전체 벡터가 들어간다.\n",
    "> - Encoder는 Feed Forward Neural Network와 Self-Attention으로 구성되어 있다.\n",
    "> - The **Self-Attention** in both encoder and decoder is the cornerstone of Transformer\n",
    "\n",
    "> - First, we represent each word with some embedding vectors\n",
    "> - Then, Transformer encodes each word to feature vectors with **Self-Attention**\n",
    "    - self-Attention은 하나의 벡터를 제외한 다른 n-1개의 벡터까지 고려해 크기다 동일할 벡터를 다음 layer로 넘겨준다.\n",
    "    - Feed-forward paths are word-independent, and paralleized\n",
    "    - **Query, Key, Value** vectors are computed per each word(=embedding)\n",
    "    - Query, Key vector의 차원은 항상 같아야 한다.\n",
    "    - **Value vector의 차원은 Q, K 차원과 동일할 필요는 없다.**\n",
    "    - 각 단어마다 해당 단어 Query 벡터와 모든 Key 벡터를 내적해 **Score** 벡터를 만들어준다.\n",
    "    - score값을 normalization 시켜준다.(key vector의 차원에 square root를 나눠준다.)\n",
    "> - Then, we compute the **attention weights** by scaling followed by softmax.\n",
    "> - The final encoding is done by the weighted sum of the **value** vectors\n",
    "> - $\\mathrm{softmax}(\\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}}) \\mathbf{V} = Z$\n",
    "\n",
    "> - 입력이 고정되어도 output의 수는 달라질 수 있다.\n",
    "> - 입력에서 전체 한번에 들어가야 된다.\n",
    "> - $n$개의 단어가 있을 때 $n^2$의 메모리 공간이 필요\n",
    "\n",
    "> - Multi-headed attention(MHA)은 attention을 여러번 하는 것\n",
    "> - $n$개의 attention을 반복하게 되면 $n$개의 encoding된 vector를 얻을 수 있다.\n",
    "> - embedding된 단어의 vector와 encoding된 output vector의 차원을 동일하게 해주어야 한다.\n",
    "    - We simply pass them through additional(learnable) linear map\n",
    "> - Positional encodings are **added** to the original embedding\n",
    "\n",
    "> - **Transformer** transfers **`key`** and **`value`** of the topmost encoder to the decoder\n",
    "> - The output sequence is generated in an autoregressive manner\n",
    "> - 학습할때는 masking을 통해 미래에 있는 정보를 활용하지 않는다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
