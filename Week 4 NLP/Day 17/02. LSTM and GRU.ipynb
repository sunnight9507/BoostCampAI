{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unavailable-stone",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory (LSTM)\n",
    "\n",
    "> - Core Idea: pass cell state information straightly without any transformation\n",
    "    - long-term dependency 문제 해결\n",
    "\n",
    "> #### Long short-term memory\n",
    "> - i: Input gate, Whether to write to cell\n",
    "> - f: Forget gate, Whether to erase cell\n",
    "> - o: Output gate, How much to reveal cell\n",
    "> - g: Gate gate, How much to write to cell\n",
    "> - $\\begin{pmatrix} i \\\\ f \\\\ o \\\\ g \\end{pmatrix} = \\begin{pmatrix} \\sigma \\\\ \\sigma \\\\ \\sigma \\\\ \\tanh \\end{pmatrix} W \\begin{pmatrix} h_{t-1} \\\\ x_t \\end{pmatrix}$\n",
    "> - $c_t = f \\odot c_{t-1} + i \\odot g$\n",
    "> - $h_t = o \\odot \\tanh{c_t}$\n",
    "\n",
    "> #### Forget gate\n",
    "> - A gate exists for controlling how much information could flow from cell state\n",
    "> - $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
    "\n",
    "> #### Input gate, Gate gate\n",
    "> - Generate information to be added and cut it by input gate\n",
    "> - $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
    "> - $\\tilde{C_t} = \\tanh{(W_C \\cdot [h_{t-1}, x_t] + b_C)}$\n",
    "> - Generate new cell state by adding current information to previous cell state\n",
    "> - $C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C_t}$\n",
    "\n",
    "> #### Output gate\n",
    "> - Generate hidden state by passing cell state to tanh and output gate\n",
    "> - Pass this hidden state to next time step, and output or next layer if needed\n",
    "> - $o_t =\\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
    "> - $h_t = o_t \\cdot \\tanh{(C_t)}$\n",
    "\n",
    "### Gated Recurrent Unit (GRU)\n",
    "\n",
    "> #### What is GRU?\n",
    "> - $z_t = \\sigma (W_z \\cdot [h_{t-1}, x_t])$\n",
    "> - $r_t = \\sigma (W_r \\cdot [h_{t-1}, x_t])$\n",
    "> - $\\tilde{h_t} = \\tanh (W \\cdot [r_t \\cdot h_{t-1}, x_t])$\n",
    "> - $h_t = (1 - z_t) \\cdot h_{t-1} + z_t \\cdot \\tilde{h_t}$\n",
    "> - c.f) $C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C_t}$ in LSTM\n",
    "\n",
    "#### Summary on RNN/LSTM/GRU\n",
    "\n",
    "> - RNNs allow a lot of **flexibility** in architecture design\n",
    "> - Vanilla RNNs are **simple** but don’t work very well\n",
    "> - Backward flow of gradients in RNN can **explode or vanish**\n",
    "> - Common to use LSTM or GRU: their additive interactions **improve gradient flow**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
