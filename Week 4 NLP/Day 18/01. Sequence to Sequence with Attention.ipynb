{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yellow-contrary",
   "metadata": {},
   "source": [
    "### Seq2Seq Model\n",
    "\n",
    "> - It takes a **sequence of words as input** and gives a **sequence of words as output**\n",
    "> - It composed of an **`encoder`** and a **`decoder`**\n",
    "\n",
    "#### Seq2Seq Model with Attention\n",
    "\n",
    "> - Attention provides a solution to the bottleneck problem\n",
    "> - Core idea: At each time step of the decoder, focus on a particular part of the source sequence\n",
    "\n",
    "> - Use the attention distribution to take a weighted sum of the encoder hidden states\n",
    "> - The attention output mostly contains information the hidden states that received high attention\n",
    "\n",
    "> - Concatenate attention output with decoder hidden state, then use to compute $\\hat{y}_1$ as before\n",
    "\n",
    "#### Different Attention Mechanisms\n",
    "\n",
    "> - **Luong** attention\n",
    "> - **Bahdanau** attention\n",
    "> - **Luong** has different types of alignments. \n",
    "> - **Bahdanau** has only a concat-score alignment model.\n",
    "> - $\\mathrm{score}(h_t, \\bar{h}_s) = \\begin{cases}\n",
    "h_t^\\top \\bar{h}_s & dot \\\\\n",
    "h_t^\\top W_a \\bar{h}_s & general \\\\\n",
    "v_a^\\top \\tanh{W_a[h_t;\\bar{h}_s]} & concat\n",
    "\\end{cases}$\n",
    "\n",
    "#### Attention is Great\n",
    "\n",
    "> - **`Attention`** significantly **improves NMT performance**\n",
    "    - It is useful to allow the decoder to focus on particular parts of the source\n",
    "> - **`Attention`** **solves the bottleneck problem**\n",
    "    - Attention allows the decoder to look directly at source; bypass the bottleneck\n",
    "> - **`Attention`** **helps with vanishing gradient problem**\n",
    "    - Provides a shortcut to far-away states\n",
    "> - **`Attention`** provides some **interpretability**\n",
    "    - By inspecting attention distribution, we can see what the decoder was focusing on\n",
    "    - The network just learned alignment by itself"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
