{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "powered-partner",
   "metadata": {},
   "source": [
    "### 1. 인공지능의 탄생과 자연어처리\n",
    "\n",
    "#### 1.1 자연어처리 소개\n",
    "\n",
    "> - 인공지능 : 인간의 `지능`이 가지는 학습, 추리, 적응, 논증 따위의 기능을 갖춘 `컴퓨터 시스템`\n",
    "\n",
    "#### 1.2 자연어처리의 응용분야\n",
    "\n",
    "> - Encoder는 Decoder가 이해할 수 있는 방법으로 `인코딩`\n",
    "> - Decoder는 본인 지식을 바탕으로 `디코딩`\n",
    "\n",
    "> - 의미 분석기\n",
    "> - 구문 분석기\n",
    "> - 감성 분석기\n",
    "> - 형태소 분석기\n",
    "> - 개채명 인식기\n",
    "\n",
    "#### 1.3 자연어 단어 임베딩\n",
    "\n",
    "> - Word2Vec\n",
    "    - 단어가 가지는 의미 자체를 다차원 공간에 `벡터화`하는 것\n",
    "    - **장점**\n",
    "        - 단어간의 유사도 측정에 용이\n",
    "        - 단어간의 관계 파악에 용이\n",
    "        - 벡터 연산을 통한 추론이 가능\n",
    "    - **단점**\n",
    "        - 단어의 subword information 무시\n",
    "        - Our of vocabulary에서 적용 불가능\n",
    "\n",
    "> - FastText\n",
    "    - 기존의 word2vec과 유사하나, 단어를 n-gram으로 나누어 학습을 수행\n",
    "    - 모든 n-gram vector를 합산한 후 평균을 통해 단어 벡터를 획득\n",
    "    - 오탈자, OOV, 등장 횟수가 적은 학습 단어에 대해서 강세\n",
    "    \n",
    "> - 단어 임베딩 방식의 한계점\n",
    "    - 동형어, 다의어 등에 대해서는 embedding 성능이 좋지 못하다는 단점이 존재\n",
    "    - 주변 단어를 통해 학습이 이루어지기 때문에, `문맥`을 고려할 수 없음\n",
    "    \n",
    "### 2. 딥러닝 기반의 자연어처리와 언어모델\n",
    "\n",
    "#### 2.1 언어모델\n",
    "\n",
    "> -`자연어`의 법칙을 컴퓨터로 모사한 모델 -> 언어`모델`\n",
    "\n",
    "> - Markov 기반의 언어모델\n",
    "    - 다음의 단어나 문장이 나올 확률을 통계와 단어의 n-gram을 기반으로 계산\n",
    "    \n",
    "> - RNN 기반의 언어모델\n",
    "\n",
    "#### 2.2 Sequence to sequence (Seq2Seq)\n",
    "\n",
    "> - Recurrent Neural Network (RNN) 기반의 Seq2Seq\n",
    "    - Encoder layer: RNN 구조를 통해 Context vector를 획득\n",
    "    - Decoder layer: 획득된 Context vector를 입력으로 출력을 예측\n",
    "    - 문제점\n",
    "        - 길이가 매우 긴 경우, 처음에 나온 token에 대한 정보가 희석\n",
    "        - 고정된 context vector 사이즈로 인해 긴 sequence에 대한 정보를 함축하기 어려움\n",
    "        - 모든 token이 영향을 미치니, 중요하지 않은 token도 영향을 줌\n",
    "        - Attention의 탄생\n",
    "        \n",
    "#### 2.3 Attention\n",
    "\n",
    "> - 중요한 feature는 더욱 중요하게 고려하는 것이 Attention의 모티브\n",
    "> - 문맥에 따라 동적으로 할당되는 encode의 Attention weight로 인한 dynamic context vector를 획득\n",
    "> - 순차적으로 연산이 이루어짐에 따라 연산 속도가 느림\n",
    "\n",
    "#### 2.4 Self-Attention\n",
    "\n",
    "> - Transformer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
