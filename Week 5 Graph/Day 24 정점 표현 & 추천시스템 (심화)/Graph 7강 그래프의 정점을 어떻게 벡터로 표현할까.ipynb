{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hispanic-continuity",
   "metadata": {},
   "source": [
    "## 1. 정점 표현 학습\n",
    "\n",
    "### 1.1 정점 표현 학습이란?\n",
    "\n",
    "> - **`정점 표현 학습`**이란 **그래프의 정점들을 벡터의 형태로 표현하는 것**이다.\n",
    "> - 정점 표현 학습은 간단히 **정점 임베딩(Node Embedding)**이라고도 부른다.\n",
    "> - 정점 임베딩은 벡터 형태의 표현 그 자체를 의미하기도 한다.\n",
    "> - 정점이 표현되는 벡터 공간을 **임베딩 공간**이라고 부르자.\n",
    "\n",
    "> - 정점 표현 학습의 입력은 그래프이다.\n",
    "> - 주어진 그래프의 각 정점 $u$에 대한 임베딩, 즉 벡터 표현 $Z_u$가 정점 임베딩의 출력입니다\n",
    "\n",
    "### 1.2 정점 표현 학습의 이유\n",
    "\n",
    "> - 정점 임베딩의 결과로, **벡터 형태의 데이터를 위한 도구들을 그래프에도 적용**할 수 있다.\n",
    "> - 기계학습 도구들이 한가지 예시이다.\n",
    "    - 대부분 **분류기**(로지스틱 회귀분석, 다층 퍼셉트론 등) 그리고 **군집 분석 알고리즘(K-Means, DBSCAN 등)**은 **벡터 형태로 표현된 사례(Instance)**들을 입력으로 받는다.\n",
    "\n",
    "> - 그래프의 정점들을 벡터 형태로 표현할 수 있다면, 위의 예시와 같은 대표적인 도구들 뿐 아니라, 최신의 기계 학습도구들을 **정점 분류(Node Classification), 군집 분석(Community Detection)** 등에 활용할 수 있다.\n",
    "\n",
    "### 1.3 정점 표현 학습의 목표\n",
    "\n",
    "> - **그래프에서의 정점간 `유사도`를 임베딩 공간에서도 보존하는 것**을 목표로 한다.\n",
    "\n",
    "> - **임베딩 공간에서의 유사도로는 `내적`(Inner Product)를 사용**한다.\n",
    "> - 임베딩 공간에서의 $u$와 $v$의 유사도는 둘의 임베딩의 내적 $\\mathbf{z_v^{\\top}z_u = ||z_u|| \\cdot ||z_v|| \\cdot \\cos{\\theta}}$이다. \n",
    "> - 내적은 두 벡터가 클 수록, 그리고 같은 방향을 향할 수록 큰 값을 갖는다.\n",
    "\n",
    "> - **정점 임베딩은 다음 `두 단계`로 이루어진다.**\n",
    "> - 1) **그래프에서의 정점 유사도를 정의**하는 단계\n",
    "> - 2) 정의한 **유사도를 보존하도록 정점 임베딩을 학습**하는 단계\n",
    "\n",
    "## 2. 인접성 기반 접근법\n",
    "\n",
    "### 2.1 인접성 기반 접근법\n",
    "\n",
    "> - **`인접성(Adjacency) 기반 접근법`**에서는 **두 정점이 인접할 때 유사**하다고 간주한다.\n",
    "> - 두 정점 $u$와 $v$가 인접하다는 것은 둘을 직접 연결하는 간선 $(u, v)$가 있음을 의미한다.\n",
    "> - **인접행렬(Adjacency Matrix) $A$**의 $u$행 $v$열 원소 $A_{u, v}$는 $u$와 $v$가 인접한 경우 $1$이 아닌 경우 $0$이다.\n",
    "> - **인접행렬의 원소 $A_{u, v}$를 두 정점 $u$와 $v$의 유사도로 가정**한다.\n",
    "\n",
    "> - 인접성 기반 접근법의 **손실 함수(Loss Function)**는 아래와 같다.\n",
    "> - 즉, 이 **손실 함수가 최소가 되는 정점 임베딩을 찾는 것**을 목표로 한다.\n",
    "> - 손실 함수 최소화를 위해서는 (확률적) 경사하강법 등이 사용된다.\n",
    "> - $L = \\sum_{(u, v) \\in V \\times V} ||z_u^{\\top}z_v - A_{u, v}||^2$\n",
    "    - 손실함수 = 모든 정점 쌍에대하여 합산 || 임베딩 공간에서의 유사도 - 그래프에서의 유사도 ||\n",
    "\n",
    "### 2.2 인접성 기반 접근법의 한계\n",
    "\n",
    "> - **인접성만으로 유사도를 판단하는 것은 한계가 있다.**\n",
    "    - 빨간색 정점과 파란색 정점은 거리가 3인 반면 초록색 정점과 파란색 정점은 거리가 2라고 가정하자.\n",
    "    - 인접성만을 고려할 경우 이러한 사실에 대한 고려 없이, 두 경우의 유사도는 0으로 같다.\n",
    "    \n",
    "## 3. 거리/경로/중첩 기반 접근법\n",
    "\n",
    "### 3.1 거리 기반 접근법\n",
    "\n",
    "> - **`거리 기반 접근법`**에서는 **두 정점 사이의 거리가 충분히 가까운 경우** 유사하다고 간주한다.\n",
    "    - 예를 들어 충분히의 기준을 2로 할 경우 거리가 2이하인 경우 유사도는 1 그렇지 않을 때는 0으로 간주한다.\n",
    "\n",
    "### 3.2 경로 기반 접근법\n",
    "\n",
    "> - **`경로 기반 접근법`**에서는 **두 정점 사이의 경로가 많을 수록** 유사하다고 간주한다.\n",
    "> - 정점 $u$와 $v$의 사이의 **경로(Path)**는 아래 조건을 만족하는 정점들의 순열(Sequence)이다.\n",
    "> - (1) $u$에서 시작해서 $v$에서 끝나야 한다.\n",
    "> - (2) 순열에서 연속된 정점은 간선으로 연결되어 있어야 한다.\n",
    "\n",
    "> - 두 정점 $u$와 $v$의 사이의 **경로** 중 거리가 $k$인 것은 수는 $A_{u, v}^k$와 같다.\n",
    "> -즉, 인접 행렬 $A$의 $k$ 제곱의 $u$행 $v$열 원소와 같다.\n",
    "> - 경로 기반 접근법의 손실 함수는 다음과 같다.\n",
    "> - $L = \\sum_{(u, v) \\in V \\times V} ||z_u^{\\top}z_v - A_{u, v}^k||^2$\n",
    "\n",
    "### 3.3 중첩 기반 접근법\n",
    "\n",
    "> - **`중첩 기반 접근법`**에서는 **두 정점이 많은 이웃을 공유할수록** 유사하다고 간주한다.\n",
    "> - 정점 $u$의 이웃 집합을 $N(u)$ 그리고 정점 $v$의 이웃 집합을 $N(v)$라고 하면 두 정점의 공통 이웃 수 $S_{u, v}$는 다음과 같이 정의된다.\n",
    "> - $S_{u, v} = |N(u) \\cap N(v)| = \\sum_{w \\in N(u) \\cap N(v)} 1$\n",
    "> - 중첩 기반 접근법의 손실 함수는 다음과 같다.\n",
    "> - $L = \\sum_{(u, v) \\in V \\times V} ||z_u^{\\top}z_v - S_{u, v}||^2$\n",
    "\n",
    "> - 공통 이웃 수를 대신 **자카드 유사도** 혹은 **Adamic Adar 점수**를 사용할 수도 있다.\n",
    "> - **자카드 유사도(Jaccard Similarity)**는 공통 이웃의 수 대신 **비율**을 계산하는 방식이다.\n",
    "    - $\\frac{|N(u) \\cap N(v)|}{|N(u) \\cup N(v)|}$\n",
    "> - **Adamic Adar 점수**는 공통 이웃 각각에 가중치를 부여하여 **가중합**을 계산하는 방식이다.\n",
    "    - $\\sum_{w \\in N(u) \\cap N(v)} \\frac{1}{d_w}$\n",
    "    \n",
    "## 4. 임의보행 기반 접근법\n",
    "\n",
    "### 4.1 임의보행 기반 접근법\n",
    "\n",
    "> - **`임의보행 기반 접근법`**에서는 **한 정점에서 시작하여 임의보행을 할 때 다른 정점에 도달할 확률**을 유사도로 간주한다.\n",
    "> - **임의보행**이란 현재 정점의 이웃 중 하나를 균일한 확률로 선택하는 이동하는 과정을 반복하는 것을 의미한다.\n",
    "> - 임의보행을 사용할 경우 시작 정점 주변의 **지역적 정보**와 **그래프 전역 정보**를 모두 고려한다는 장점이 있다.\n",
    "\n",
    "> - 임의보행 기반 접근법은 **세 단계**를 거친다.\n",
    "> - 1) 각 정점에서 시작하여 임의보행을 반복 수행한다.\n",
    "> - 2) 각 정점에서 시작한 임의보행 중 도달한 정점들의 리스트를 구성한다.\n",
    "> - 이 때, **정점 $u$에서 시작한 임의보행 중 도달한 정점들의 리스트를 $N_R(u)$**라고 하자.\n",
    "> - 한 정점을 여러 번 도달한 경우, 해당 정점은 $N_R(u)$에 여러 번 포함될 수 있다.\n",
    "> - 3) 다음 손실함수를 최소화하는 임베딩을 학습한다.\n",
    "> - $L = \\sum_{u \\in V} \\sum_{v \\in N_R(u)} - \\log{(P(v|z_u))}$\n",
    "    - $u$에서 시작한 임의보행이 $v$에 도달할 확률을 임베딩으로부터 추정한 결과를 의미한다.\n",
    "    \n",
    "> - **정점 $u$에서 시작한 임의보행이 정점 $v$에 도달할 확률 $P(v|z_u)$**을 다음과 같이 추정한다.\n",
    "> - $P(v|z_u) = \\frac{\\exp{(z_u^{\\top}z_v)}}{\\sum_{n \\in V}  \\exp{(z_u^{\\top}z_v)}}$\n",
    "> - 즉 유사도 $z_u^{\\top}z_v$가 높을 수록 도달 확률이 높다.\n",
    "\n",
    "### 4.2 DeepWalk와 Node2Vec\n",
    "\n",
    "> - 임의보행의 방법에 따라 **DeepWalk**와 **Node2Vec**이 구분된다.\n",
    "> - **DeepWalk**는 앞서 설명한 **기본적인 임의보행을 사용**한다.\n",
    "> - 즉, 현재 정점의 이웃 중 하나를 **균일한 확률로 선택**해 이동하는 과정을 반복한다.\n",
    "\n",
    "> - **Node2Vec**은 **2차 치우친 임의보행(Second-order Biased Random Walk)**을 사용한다.\n",
    "> - **현재 정점**과 **직전에 머물렀던 정점**을 모두 고려하여 다음 정점을 선택한다.\n",
    "> - 직전 정점의 거리를 기준으로 경우를 구분하여 차등적인 확률을 부여한다.\n",
    "\n",
    "> - **Node2Vec에서는 부여하는 확률에 따라서 다른 종류의 임베딩을 얻는다.**\n",
    "> - **멀어지는 방향에 높은 확률**을 부여한 경우, **정점의 역할**(다리 역할, 변두리 정점 등)이 같은 경우 임베딩이 유사하다.\n",
    "> - **가까워지는 방향에 높은 확률**을 부여한 경우, **같은 군집**(Community)에 속한 경우 임베딩이 유사하다.\n",
    "\n",
    "### 4.3 손실 함수 근사\n",
    "\n",
    "> - **임의보행 기법의 손실함수는 계산에 정점의 수의 `제곱`에 비례하는 시간이 소요된다.**\n",
    "> - 중첩된 합 때문에 이러한 시간이 소요된다.\n",
    "\n",
    "> - 따라서 많은 경우 **근사식을 사용**한다.\n",
    "> - **모든 정점에 대해서 정규화**하는 대신 **몇 개의 정점을 뽑아서 비교**하는 형태이다.\n",
    "> - 이 때 뽑힌 정점들을 **네거티브 샘플**이라고 부른다.\n",
    "> - $\\log{\\frac{\\exp{(z_u^{\\top}z_v)}}{\\sum_{n \\in V}  \\exp{(z_u^{\\top}z_v)}}} \\thickapprox \\log{(\\sigma(z_u^\\top z_v)) - \\sum_{i=1}^k \\log{(\\sigma(z_u^\\top z_v))}, n_i \\sim P_V}$\n",
    "> - **연결성에 비례하는 확률**로 **네거티브 샘플**을 뽑으며, 네거티브 샘플이 많을 수록 학습이 더욱 안정적이다.\n",
    "\n",
    "## 5. 변환식 정점 표현 학습의 한계\n",
    "\n",
    "### 5.1 변환식 정점 표현 학습과 귀납식 정점 표현 학습\n",
    "\n",
    "> - 지금까지 소개한 정점 임베딩 방법들을 **변환식(Transductive) 방법**이다.\n",
    "> - **변환식(Transdctive)** 방법은 **학습의 결과로 정점의 임베딩 자체**를 얻는다는 특성이 있다.\n",
    "> - **정점을 임베딩으로 변화시키는 함수**, 즉 **인코더**를 얻는 **귀납식(Inductive) 방법**과 대조됩니다\n",
    "\n",
    "### 5.2 변환식 정점 표현 학습의 한계\n",
    "\n",
    "> - **변환식 임베딩 방법은 여러 한계를 갖는다.**\n",
    "> - 1) 학습이 진행된 이후에 추가된 정점에 대해서는 임베딩을 얻을 수 없다.\n",
    "> - 2) 모든 정점에 대한 임베딩을 미리 계산하여 저장해두어야 한다.\n",
    "> - 3) 정점이 속성(Attribute) 정보를 가진 경우에 이를 활용할 수 없다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
