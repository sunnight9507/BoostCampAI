{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "structural-subdivision",
   "metadata": {},
   "source": [
    "### 1. 자연어 전처리\n",
    "\n",
    "> - **`전처리`** : 학습에 사용될 데이터를 **수집 & 가공**하는 모든 프로세스\n",
    "\n",
    "#### 1.1 자연어처리의 단계\n",
    "\n",
    "> - Task 설계\n",
    "> - 필요 데이터 수집\n",
    "> - 통계학적 분석\n",
    "    - Token 개수 -> 아웃라이어 제거\n",
    "    - 빈도 확인 -> 사전(dictionary) 정의\n",
    "> - 전처리\n",
    "    - 개행문자 제거\n",
    "    - 특수문자 제거\n",
    "    - 공백 제거\n",
    "    - 중복 표현 제어\n",
    "    - 이메일, 링크 제거\n",
    "    - 제목 제거\n",
    "    - 불용어 (의미 없는 용어) 제거\n",
    "    - 조사 제거\n",
    "    - 띄어쓰기, 문장분리 보정\n",
    "> - Tagging\n",
    "> - Tokenizing : 자연어를 어떤 단위로 살펴볼 것인가\n",
    "    - 어절 / 형태소 / WordPiece\n",
    "> - 모델 설계\n",
    "> - 모델 구현\n",
    "> - 성능 평가\n",
    "> - 완료\n",
    "\n",
    "#### 1.2 Python string 관련 함수\n",
    "\n",
    "> - 대소문자의 변환\n",
    "    - captitalize()\n",
    "    - title()\n",
    "    - swapcase()\n",
    "> - 구성 문자열 판별\n",
    "    - isdigit()\n",
    "    - isalpha()\n",
    "    - isalnum()\n",
    "    - isspace()\n",
    "> - 검색\n",
    "    - count(\"string\")\n",
    "    - find(\"string\")\n",
    "    - rfind(\"string\")\n",
    "    - index(\"string\")\n",
    "    - rindex(\"string\")\n",
    "    \n",
    "### 2. 한국어 토큰화\n",
    "\n",
    "#### 2.1 한국어 토큰화\n",
    "\n",
    "> - **토큰화**(Tokenizing)\n",
    "> - **문장 토큰화**(Sentence Tokenizing)\n",
    "> - **단어 토큰화**(Word Tokenizing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-failure",
   "metadata": {},
   "source": [
    "### 새로 학습한 내용\n",
    "\n",
    "> - [전후방탐색](https://blog.hexabrain.net/205)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "foreign-fitness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.sub(\"<[^>]+>\\s+(?=<)|<[^>]+>\", \"\", \"<text>   \\n\\t   <text>text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "authentic-retail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   \\n\\t   text'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"<[^>]+>\", \"\", \"<text>   \\n\\t   <text>text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-department",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> - 중복된 문장을 제거할 때 자료형 **`set`**이 아닌 **`OrderedDict`** 사용하는 이유\n",
    "    - Text 데이터이므로 **순서**가 보존되어야 한다.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-conflict",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-brook",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-capital",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-louis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-michigan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-ethernet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
