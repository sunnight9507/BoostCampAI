{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electronic-velvet",
   "metadata": {},
   "source": [
    "#### Sequence Data\n",
    "\n",
    "> - 소리, 문자열, 주가 등의 데이터를 시퀀스(sequence) 데이터로 분류한다.\n",
    "> - 시계열 데이터는 시간 순서에 따라 나열된 데이터로 시퀀스 데이터에 속한다.\n",
    "> - 시퀀스 데이터는 독립동등분포(i.i.d.) 가정을 잘 위배하기 때문에 순서를 바꾸거나 과거 정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 된다.\n",
    "\n",
    "#### Sequence Data 다루기\n",
    "\n",
    "> - 이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률을 이용할 수 있다.\n",
    "> - $P(X_1,\\dots,X_t) = P(X_t|X_1,\\dots,X_{t-1})P(X_1,\\dots,X_{t-1}) = \\prod_{s=1}^t P(X_s|X_{s-1},\\dots,X_1)$\n",
    "> - $X_t \\thicksim P(X_t|X_{t-1},\\dots,X_1)$\n",
    "    - 위 조건부확률은 과거의 모든 정보를 사용하지만 시퀀스 데이터를 분석할 때 모든 과거 정보들이 필요한 것은 아니다.\n",
    "> - 시퀀스 데이터를 다루기 위해서는 **길이가 가변적인 데이터를 다룰 수 있는 모델이 필요**하다.\n",
    "    - 고정된 길이 $\\tau$만큼의 시퀀스만 사용하는 경우 $AR(\\tau)$(Autoregressive Model) 자기회귀모델이라고 부른다.\n",
    "    - 또 다른 방법은 바로 이전 정보를 제외한 나머지 정보들을 $H_t$라는 잠재변수로 인코딩해서 활용하는 잠재 AR모델이다.\n",
    "> - $X_t \\thicksim P(X_t|X_{t-1},H_t), \\quad H_t = \\mathbf{Net}_\\theta (H_{t-1}, X_{t-1})$ \n",
    "> - **잠재변수 $H_t$를 신경망을 통해 반복해서 사용하여 시퀀스 데이터의 패턴을 학습하는 모델이 `RNN`이다.**\n",
    "\n",
    "#### Recurrent Neural Network 이해하기\n",
    "\n",
    "> - 가장 기본적인 RNN 모형은 MLP와 유사한 모양이다.\n",
    "> - $\\mathbf{O}_t = \\mathbf{H_t W}^{(2)} + \\mathbf{b}^{(2)}$\n",
    "> - $\\mathbf{H}_t = \\sigma(\\mathbf{X_t W}^{(1)} + \\mathbf{b}^{(1)})$\n",
    "    - $\\mathbf{H}$ : 잠재변수\n",
    "    - $\\sigma$ : 활성화함수\n",
    "    - $\\mathbf{W}$ : 가중치행렬\n",
    "    - $\\mathbf{b}$ : bias\n",
    "    - $\\mathbf{W}^{(1)}, \\mathbf{W}^{(2)}$는 시퀀스와 상관없이 불변인 행렬이다.\n",
    "    - 해당 모델은 과거의 정보를 다룰 수 없다.\n",
    "\n",
    "> - RNN은 이전 순서의 잠재변수와 현재의 입력을 활용해 모델링한다.\n",
    "> - $\\mathbf{O}_t = \\mathbf{H_t W}^{(2)} + \\mathbf{b}^{(2)}$\n",
    "> - $\\mathbf{H}_t = \\sigma(\\mathbf{X_t W_X}^{(1)} + \\mathbf{H}_{t-1}\\mathbf{W}_H^{(1)}  + \\mathbf{b}^{(1)})$\n",
    "    - 잠재변수인 $\\mathbf{H}_t$를 복제해서 다음 순서의 잠재변수를 인코딩하는데 사용한다.\n",
    "> - RNN의 역전파는 잠재변수의 연결그래프에 따라 순차적으로 계산한다.\n",
    "    - 이를 **Backpropagation Through Time(BPTT)**이라 하며 RNN의 역전파 방법이다.\n",
    "    \n",
    "#### BPTT\n",
    "\n",
    "> - BPTT를 통해 RNN의 가중치행렬의 미분을 계산해보면 아래와 같이 미분의 곱으로 이루어진 항이 계산된다.\n",
    "> - $L(x, y, w_h, w_o) = \\sum_{t=1}^T \\ell(y_t, o_t)$\n",
    "> - $h_t = f(x_t, h_{t-1}, w_h)\\ \\mathrm{and}\\ o_t = g(h_t, w_o)$\n",
    "    - 잠재변수의 미분 term이 곱해지게 된다.\n",
    "    - 시퀀스 길이가 길어질수록 해당 곱해지는 항이 불안정해질 수 있다.\n",
    "    \n",
    "#### 기울기 소실의 해결책?\n",
    "\n",
    "> - 시퀀스 길이가 길어지는 경우 BPTT를 통한 역전파 알고리즘의 계산이 불안정해지므로 길이를 끊는 것이 필요하다.\n",
    "    - 이를 truncated BPTT라 부른다.\n",
    "> - 이런 문제들 때문에 Vanila RNN은 길이가 긴 시퀀스를 처리하는데 문제가 있다.\n",
    "> - 이를 해결하기 위해 등장한 네트워크가 **`LSTM`**과 **`GRU`**이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
