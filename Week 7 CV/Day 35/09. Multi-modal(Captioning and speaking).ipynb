{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "friendly-methodology",
   "metadata": {},
   "source": [
    "## 1. Overview of multi-modal learning\n",
    "\n",
    "> - Modalities in multi-modal learning\n",
    "\n",
    "> - Challenge (1) - Different representations between modalities\n",
    "> - Challenge (2) - Unbalance between heterogeneous feature spaces\n",
    "> - Challenge (3) - May a model be biased on a specific modality\n",
    "\n",
    "> - Despite the challenges, multi-modal learning is fruitful and important\n",
    "    - Matching\n",
    "    - Translating\n",
    "    - Referencing\n",
    "    \n",
    "## 2. Multi-modal tasks (1) - Visual data & Text\n",
    "\n",
    "### 2.1 Text embedding\n",
    "\n",
    "> - Map to dense vectors\n",
    "> - Generalization power is obtained by learning dense representation\n",
    "\n",
    "> - word2vec - Skip-gram model\n",
    "    - Learning to predict neighboring $N$ words for understanding relationships between words\n",
    "    \n",
    "### 2.2 Joint embedding\n",
    "\n",
    "> - Image tagging\n",
    "    - Can generate tags of a given image, and retrieve images by a tag keyword as well\n",
    "    - Combining pre-trained unimodal models\n",
    "    - Metric learning : Joint visual-semantic embedding space\n",
    "    - Multi-modal analogy\n",
    "    - Image & food recipe retriecal\n",
    "    \n",
    "### 2.3 Cross modal translation\n",
    "\n",
    "> - Image captioning\n",
    "    - Captioning as image-to-sentence - CNN for image & RNN for sentence\n",
    "    \n",
    "> - Show and tell\n",
    "    - Encoder : CNN model pre-trained on ImageNet\n",
    "    - Decoder : LSTM module\n",
    "    \n",
    "> - Show, attend, and tell\n",
    "    - 1) Input Image\n",
    "    - 2) Convolutional Feature Extraction\n",
    "    - 3) RNN with attention over the image\n",
    "    - 4) Word by word generation\n",
    "    \n",
    "> - Text-to-images by generative model\n",
    "\n",
    "### 2.4 Cross modal reasoning\n",
    "\n",
    "> - Visual question answering\n",
    "    - Multiple streams\n",
    "    - Joint embedding\n",
    "    - End-to-end training\n",
    "    \n",
    "## 3. Multi-modal tasks (2) - Visual data & Audio\n",
    "\n",
    "### 3.1 Sound representation\n",
    "\n",
    "> - Acoustic feature extraction from waveform to sepctogram\n",
    "\n",
    "> - Fourier transform\n",
    "    - Short-time Fourier transform (STFT)\n",
    "        - Fourier transform (FT) on windowed waveform results in frequency-magnitude graph\n",
    "    - FT decomposes an input signal into consitituent frequencies\n",
    "\n",
    "> - Spectrogram\n",
    "    - A stack of spectrums along the time axis\n",
    "    \n",
    "### 3.2 Joint embedding\n",
    "\n",
    "> - Scene recognition by sound\n",
    "\n",
    "> - SoundNet\n",
    "    - Learn audio representation from synchronized RGM frames in the same videos\n",
    "    - Train by the teacher-student manner\n",
    "    \n",
    "### 3.3 Cross modal translation\n",
    "\n",
    "> - Speech2Face\n",
    "    - Training by feature matching loss (self-supervised manner) for making features compatible\n",
    "    \n",
    "> - Image-to-speech synthesis\n",
    "    - Image-to-Unit Model (Show, Attend, and Tell)\n",
    "        - Image captioning, but sub-word units not natural language\n",
    "    - Unit-to-Speech Model (TTS)\n",
    "    - Speech-to-Unit Model (ResDAVEnet-VQ)\n",
    "    \n",
    "### 3.4 Cross modal reasoning\n",
    "\n",
    "> - Sound source localization\n",
    "    - Visual network -> feature map\n",
    "    - Audio network -> Sound feature vector\n",
    "    - Attention network\n",
    "    \n",
    "> - Looking to listen at the cocktail party"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
